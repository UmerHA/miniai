# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_model_blocks.ipynb.

# %% auto 0
__all__ = ['conv', 'saved', 'DownBlock', 'UpBlock', 'EmbUNetModel']

# %% ../nbs/05_model_blocks.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np
import pandas as pd,matplotlib.pyplot as plt
from functools import partial, wraps
from pathlib import Path
from torch import tensor
from torch import nn
import fastcore.test as fct

from torch.utils.data import DataLoader,default_collate
from typing import Mapping

from .datasets import *

# %% ../nbs/05_model_blocks.ipynb 8
def conv(ni, # Input filters
         nf, # Output filters
         ks=3, # Kernel size
         stride=2, # Stride,
         padding=None, # Padding
         act=nn.ReLU, # Activation
         norm=None, # Type of normalization layer to apply
         bias=None # Whether to apply bias
        )-> nn.Sequential:
    """ Generate a conv block with a conv layer and optional normalisation and activation. If bias 
    is None then the bias is not applied  to the conv if batch norm is used, otherwise it is

    Using ks=3 and padding=1 will result in the resolution reducing as per the stride, as will 
    ks=5 and padding=2

    Returns the block as a sequential model
    """
    if bias is None:
        bias = not norm in (torch.nn.modules.batchnorm.BatchNorm1d, 
                            torch.nn.modules.batchnorm.BatchNorm2d, 
                            torch.nn.modules.batchnorm.BatchNorm3d)
    if padding is None: padding=ks//2
    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=padding, bias=bias)]
    if norm: layers.append(norm(nf))
    if act: layers.append(act())
    return nn.Sequential(*layers)

# %% ../nbs/05_model_blocks.ipynb 26
def _conv_block(ni, # input channels
                nf, # out channels
                stride, # stride
                ks=3, # kernel size
                act=act_gr, # activation to use
                norm=None # normalization to use
               ):
    """ Generates a sequential model consisting of two conv blocks.  Note that the architectual 
    choice being made here is that the first conv changes the number of channels and the second 
    keeps the number of channels the same but reduces the resolution by using stride=2

    """
    return nn.Sequential(
        conv(ni, nf, stride=1, ks=ks, act=act, norm=norm),
        conv(nf, nf, stride=stride, ks=ks, act=None, norm=norm)
    )

# %% ../nbs/05_model_blocks.ipynb 44
def saved(m, # torch.nn.module, the module for which the output will be saved
          blk # The block containing the module
         ):
    """Creates a function that will save the values of the embedding layers to facilitate passing to the 
    decoding part.  Depends upon the calling block containing a 'saved" attribute as a list. Each module in the block 
    for which this is called will have its exit activations saved in the list

    The wraps library simply ensures that any documentation of the wrapped method is available to the parent
    """
    m_ = m.forward

    @wraps(m.forward)
    def _f(*args, **kwargs):
        res = m_(*args, **kwargs)
        blk.saved.append(res)
        return res

    m.forward = _f
    return m

# %% ../nbs/05_model_blocks.ipynb 53
class DownBlock(nn.Module):
    """ A down block is a part of a stable diffusion Unet.  It contains an EmbResBlock which is followed 
    by an optional down block (if no down block then an identity is used).  Activations of teh EmbResBlock
    and the down block are saved for use as cross connections for the corresponding up blocks
    """
    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):
        super().__init__()
        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)
                                      for i in range(num_layers)])
        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()

    def forward(self, x, t):
        self.saved = []
        for resnet in self.resnets: x = resnet(x, t)
        x = self.down(x)
        return x

# %% ../nbs/05_model_blocks.ipynb 54
class UpBlock(nn.Module):
    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):
        super().__init__()
        self.resnets = nn.ModuleList(
            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)
            for i in range(num_layers)])
        self.up = upsample(nf) if add_up else nn.Identity()

    def forward(self, x, t, ups):
        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)
        return self.up(x)

# %% ../nbs/05_model_blocks.ipynb 55
class EmbUNetModel(nn.Module):
    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1, attn_chans=8, attn_start=1):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)
        self.n_temb = nf = nfs[0]
        n_emb = nf*4
        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),
                                     lin(n_emb, n_emb))
        self.downs = nn.ModuleList()
        n = len(nfs)
        for i in range(n):
            ni = nf
            nf = nfs[i]
            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=n-1, num_layers=num_layers,
                                        attn_chans=0 if i<attn_start else attn_chans))
        self.mid_block = EmbResBlock(n_emb, nfs[-1])

        rev_nfs = list(reversed(nfs))
        nf = rev_nfs[0]
        self.ups = nn.ModuleList()
        for i in range(n):
            prev_nf = nf
            nf = rev_nfs[i]
            ni = rev_nfs[min(i+1, len(nfs)-1)]
            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=n-1, num_layers=num_layers+1,
                                    attn_chans=0 if i>=n-attn_start else attn_chans))
        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)

    def forward(self, inp):
        x,t = inp
        temb = timestep_embedding(t, self.n_temb)
        emb = self.emb_mlp(temb)
        x = self.conv_in(x)
        saved = [x]
        for block in self.downs: x = block(x, emb)
        saved += [p for o in self.downs for p in o.saved]
        x = self.mid_block(x, emb)
        for block in self.ups: x = block(x, emb, saved)
        return self.conv_out(x)
